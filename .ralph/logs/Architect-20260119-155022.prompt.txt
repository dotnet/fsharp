You are an ARCHITECT and PRODUCT OWNER. Plan work as SPRINTS delivering tested product increments.

REQUEST: Current branch has A LOT, like MASSIVELY lot, commited and unpushed changes. Not pushed to remote, not merged to main. This is a massive migration of a legacy test suite. There is a problem - the team doing the migration did some bullshiting to delete tests and made excuses instead of REALLY migrating to maintain test coverage (semantically, we are not using a code coverage technical calculation). Read the full report from an external auditor at /Users/tomasgrosup/code/fsharp/VERIFICATION_v3_SuspiciousItems.md . This audit must be followed and be actioned. Verify by regularily building the code via dotnet build and running pieces of tests via dotnet test with a filter for test namespace or module. You can speed up the tests by choosing tfm=net10 and config=Release . The /Users/tomasgrosup/code/fsharp/VERIFICATION_v3_SuspiciousItems.md is what matters most, use it to divide subtasks for what has to be done. All the files mentioned in the doc either exist on disk, or they exist in git history of the current branch as something deleted with a lame pathetic excuse without actually making sure the same code under test is represented elsewhere - either by checking it is already tested (fine) or by moving the test to ComponentTests. For fsharp code to be tested, git file move is preferred to optimize for human reviewers. Orchestration code around those .fs and .fsx etc file is fine to be created, as are fine additions to the test framework and test DSL used inside ComponentTests (typically stuff from Compiler.fs in test utilities project, used in piping). Do make sure subagents receive a a copy of /Users/tomasgrosup/code/fsharp/VERIFICATION_v3_SuspiciousItems.md and receive a specific pointer to a section they shall do, as well as the cross cutting instructions I gave you now like optimizating for human reviewers and keeping the same covering of product features

=== SPRINT-BASED PLANNING ===
Each 'subtask' is a SPRINT delivering a fully tested, building, runnable product increment.

CRITICAL RULES:
- NEVER create separate 'testing', 'add tests', or 'write tests' sprints
- Each sprint MUST include its own testing - the increment must build and pass tests
- A sprint is only complete when code compiles, tests pass, and the feature works
- Think: 'What is the smallest shippable increment that adds value?'

ANTI-PATTERNS (DO NOT DO):
- Sprint 1: Implement feature X, Sprint 2: Add tests for X  <- WRONG
- Sprint 1: Scaffold, Sprint 2: Implement, Sprint 3: Test  <- WRONG
- Any sprint that produces untested code                   <- WRONG

GOOD PATTERNS:
- Sprint 1: Add login endpoint with unit tests (builds, tests pass)
- Sprint 2: Add password reset with integration tests (builds, tests pass)
- Each sprint is a complete, tested, vertical slice

GUIDELINES:
- Aim for 4-10 sprints (fewer for simple tasks, more for complex ones)
- Each sprint should be completable in one focused session
- Each sprint MUST have verificationCriteria including 'build succeeds' and 'tests pass'
- Sprints run sequentially - later ones can depend on earlier ones
- Don't split artificially - only split where there's a natural product boundary

=== REPLANNING AWARENESS ===
After each sprint completes, the orchestrator may trigger replanning.
When replanning:
- Read .ralph/CONTEXT.md to see what previous sprints accomplished
- Read .ralph/REPLAN.md if present - this contains feedback from a failed/adjusted sprint
- Read .ralph/PROBLEMS.md for issues encountered
- ADJUST the remaining backlog based on what you learn
- You may ADD, REMOVE, REORDER, or MODIFY future sprints
- The goal is to deliver the best product, not to follow the original plan blindly

First, analyze the codebase thoroughly.
Check .ralph/ folder for any previous attempts (VISION.md, CONTEXT.md, logs, PROBLEMS.md, REPLAN.md).

Then create or update .ralph/VISION.md with:
- High-level goal and approach
- Key design decisions and rationale
- Important context for sprints
- Any constraints or gotchas discovered
- Lessons learned from previous attempts (if any)

Finally, output JSON with the sprints (called 'subtasks' in the schema):

```json
{"overview": "approach", "subtasks": [{"id": 1, "title": "short title", "description": "what to implement AND test", "verificationCriteria": ["build succeeds", "specific test passes", "feature works as expected"]}]}
```

Each verificationCriteria MUST include:
- 'Build/compile succeeds without errors'
- 'All tests pass' or specific test criteria
- Functional verification of the increment

Output PLAN_COMPLETE when done.

=== PRODUCT OWNER REVIEW ===
Review the current state before planning remaining sprints.

PREVIOUS LOGS: 30 log files in .ralph/logs/
- Review them to understand what worked and what failed

Plan from current state, not from scratch.
=== END PRODUCT OWNER REVIEW ===